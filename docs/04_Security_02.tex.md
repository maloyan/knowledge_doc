Методы генерации вредоносных искажений. $L_p$ норма. Атаки белого ящика. Атаки черного ящика. Отравление данных. {#4.02}
----------------------------------------------------------------------------------------------------------------

$L_p$ - незаметность. В силу небольших изменений, человек не в состоянии
замечать возмущений во входных данных.

$||x||_p=(|x_1|^p+|x_2|^p+...+|x_n|^p)^{1/p}$

$||x||_{\inf}=max\{|x_1|,|x_2|,...,|x_n|\}$

-   Незаметность внимания, когда в силу малости изменений человек
    практически не должен замечать изменений. Для *изображений* здесь
    обычно упоминают повороты и трансляции.

-   Незаметность изменения *результатов*. Человек-наблюдатель не может
    легко заметить отклонения в результатах.

-   Незаметность для *детекторов*. При этом детекторы могут искать
    возмущения в исходных данных, в результатах и в шаблонах активации
    (то есть, в работе сети).

**Норма $L_0$**: Атака с ограничением нормы L0 обычно включает в себя
изменение определенного количества характеристик входного сигнала
модели. Атаки с ограничением $L_0$-нормы часто очень реалистичны и могут
быть запущены в реальных системах. (Типичный пример - наклейка,
добавленная к знаку остановки, которая может заставить беспилотный
автомобиль не замедляться - весь фон сохраняется, и изменяется только
крошечная часть окружающей среды.)

**Норма $L_$**: Атака с ограничением по норме $L_1$ включает ограничение
сверху суммы общих значений возмущения. Эта атака довольно необычна.

**Норма $L_2$**: Атака с ограничением по норме $L_2$ включает
ограничение сверху евклидова расстояния / пифагорова расстояния
возмущения $d$. Атаки с ограничением по норме $L_2$ используются
довольно часто из-за математической значимости норм $L_2$ в линейной
алгебре и геометрии.

**Норма $L_{\inf}$**: это значение максимальной разницы среди пикселей.
Была обнаружена первой, такие атаки изучаются больше всего из-за их
простоты и математического удобства при надежной оптимизации.

Атака -- это сознательная генерация (подбор) состязательных примеров,
которые существуют (могут существовать) и без злонамеренных
пользователей.

Отсюда -- важно понимать природу данных ("физику" системы) и возможность
оценки генеральной совокупности.

### Атаки белового ящика

White-box (белый ящик) -- атакующий имеет полную информацию о модели
(архитектура, веса, стратегии тренировки и т.д.)

злоумышленник может вычислить функцию потерь модели J(θ, X, y) где X -
входное изображение, y - выходной класс, а θ - внутренние параметры
модели.

Эта функция потерь обычно является вероятностью отрицательной потери для
методов классификации

Все методы атак белого ящика пытаются максимизировать изменение функции
потерь модели, сохраняя при этом небольшое возмущение входного
изображения.

Заметка: Чем выше размерность пространства входного изображения, тем
легче создавать состязательные примеры, неотличимые от чистых
изображений человеческим глазом.

$x$ - вход модели, $\ell$ - результат классификации, $x_0$ - порожденный
вход, $\ell_0$ - результат классификации порожденного входа,
$L(x, \ell)$ - функция потерь

Атаки на основе градиента линеаризуют потери (например, кросс-энтропию)
вокруг входа $x$, чтобы найти направления $\rho$, к которому прогнозы
модели для класса $\ell$ наиболее чувствительны

$L(x+\rho,\ell) \approx L(x,\ell)+\rho T \nabla_x L(x,\ell)$

$\nabla_x L(x,\ell)$ - градиент функции потерь по входу $x$.

Градиент используется для определения направления. Так как из
backpropogation выходной градиент определяет и входные градиенты
(направления изменения входных параметров).

**Gradient Attack** Атака вычисляет градиент:
$g(x_0)=\nabla_x L(x_0, \ell_0)$, а далее ищет минимальное значение
(шаг) $\epsilon$ так, что $x_0+\epsilon g(x_0)$ -- состязательный пример
(на нем будет ощибочная классификация)

**Gradient Sign Attack(FGSM)** Атака вычисляет градиент:
$g(x_0)=\nabla_x L(x_0, \ell_0)$, а далее ищет минимальное значение
(шаг) $\epsilon$ так, что $x_0+\epsilon \operatorname{sign}(g(x_0))$ --
состязательный пример

FGSM заключается в добавлении шума (а не случайного шума), направление
которого совпадает с градиентом функции стоимости по отношению к данным.
Шум масштабируется по параметру $\epsilon$. В этой формуле имеет
значения величина не величина градиента, а его направление.

Это похоже на градиентный спуск, который предназначен для обновления
весов модели, чтобы минимизировать функцию стоимости путем получения
градиента по отношению к весу.

**Iterative Gradient Attack** Потери максимизируются на небольших
ступенях в направлении $g(x)=\nabla_x L(x, \ell_0)$. Итеративно
обновляется $x_{k+1} \xleftarrow[]{} x_k+\epsilon g(x_k)$

$\epsilon$ настраивается внутри, чтобы найти минимальное возмущение.

**Iterative Gradient Sign Attack**
$g(x_0)=\operatorname{sign}(\nabla_x L(x_0, \ell_0))$ --
$x_{k+1} \xleftarrow[]{} x_k+\epsilon g(x_k)$

**DeepFool Attack** На каждой итерации DeepFool вычисляет для каждого
класса $\ell \neq \ell_0$ минимальное расстояние $d(\ell, \ell_0)$,
необходимое для достижения границы класса, путем аппроксимации
классификатора модели линейным классификатором. Затем он делает
соответствующий шаг в направлении класса с наименьшим расстоянием.

**Jacobian-Based Saliency Map Attack** Эта целевая атака использует
градиент для вычисления оценки значимости для каждого входного параметра
(например, пикселя). Этот *показатель значимости отражает*, насколько
сильно каждая функция может подтолкнуть классификацию модели от
эталонного к целевому классу. Этот *процесс повторяется*, и на каждой
итерации нарушается только функция с максимальной оценкой значимости.

### Атаки черного ящика

-   Черный ящик с прозрачным выходом -- структура модели неизвестна, но
    есть полный доступ к результатам работы

-   Черный ящик с ограниченными запросами -- структура модели
    неизвестна, есть полный доступ к результатам работы, но либо
    количество запросов, либо их частота ограничены

-   Черный ящик с ограниченными результатами. Структура модели
    неизвестна, доступна только часть выходных результатов. Чаще всего
    -- классы без вероятностей (для задач классификации, которые и
    являются наиболее частым объектом атак).

-   Черный ящик. Структура модели неизвестна, результаты работы
    недоступны. Атакующий должен создать атаку без обращений к атакуемой
    системе.

Во многих работах неоднократно отмечалось, что состязательные примеры
довольно хорошо передаются между моделями, а это означает, что они могут
быть разработаны для целевой модели A, но в конечном итоге эффективны
против любой другой модели, обученной на аналогичном наборе данных.

**Аддитивная равномерная шумовая атака** -- Эта атака проверяет
надежность модели на добавление равномерного шума. Внутренний поиск
выполняется для нахождения минимальных посторонних возмущений.

**Аддитивная шумовая атака по Гауссу** -- Эта атака проверяет надежность
модели на добавление нормального шума. Внутренний поиск выполняется для
нахождения минимальных посторонних возмущений.

**Шумовая атака Salt And Pepper** -- Эта атака проверяет надежность
модели на добавление специального шума (соли -- белые пиксели и перца --
черные пиксели). Внутренний поиск выполняется для нахождения минимальных
посторонних возмущений.

**Атака уменьшения контрастности** -- Эта атака проверяет устойчивость
модели к снижению контрастности. Внутренний поиск выполняется для
нахождения минимальных посторонних возмущений.

***Внутренний поиск*** может выполняться следующим образом: 1) строится
дискретная сетка параметров, значения считаются по сетке и выбирается
наихудшее 2) случайно выбираются k значений параметра. Из результатов
выбирается худший

**Размытие по Гауссу** (проверка устойчивости к размытию по Гауссу)

**Однопиксельная атака** (устойчивость к изменению отдельных пикселей,
устанавливая для одного пикселя черный или белый цвет)

**Атака на локальный поиск** -- измеряет чувствительность модели к
отдельным пикселям, применяя экстремальные возмущения и наблюдая влияние
на вероятность правильного класса, так ищутся наиболее чувствительные
пиксели, и делается состязательное изображение.

### Отравление данных

Атаки с отравлением данных, напротив, могут манипулировать процессом
обучения. Атакующие стремятся манипулировать обучающими данными
(например, изменять метки, манипулировать настройками конфигурации
модели, изменять веса модели и т.п.), чтобы повлиять на результат
обучения модели.

**Переворачивание меток**

Простой и эффективный способ атаковать процесс обучения -- поменять
местами метки некоторых экземпляров обучения.

Атака с переворачиванием *случайных* меток не зависит от модели.
Алгоритм просто выбирает подмножество обучающих экземпляров и
переворачивает (изменяет) их метки.

Несмотря на то, что эта случайная стратегия выглядит простой, она
способна уменьшить точность классификации.

Атаки со случайным переворачиванием меток можно разделить на две группы:
*целевые* и *нецелевые*.

В *нецелевой* атаке со случайным переключением меток, атакующий может
выбрать некоторые экземпляры из класса A, чтобы ошибочно
классифицировать их как класс B и некоторые экземпляры из класса B,
чтобы ошибочно классифицировать их как класс A.

В *целевой* атаке, злоумышленник последовательно ошибочно классифицирует
один класс как другой. Целевая атака случайного переключение меток
является более серьезной по сравнению с нецелевой, поскольку целевая
атака постоянно вводит в заблуждение алгоритм обучения для классификации
определенного класса экземпляров как другого определенного класса.

Вместо случайного переключения меток злоумышленник также может
использовать атаки с переворотом меток, которые зависят от модели.

Например, можно попробовать изменить гиперплоскость в SVM, воздействуя
на (неверно классифицируя) вектора, которые находятся "далеко" от
гиперплоскости (то есть те, которые без атаки правильно
классифицировались).
