Методы оптимизации. Градиентный спуск, SGD, AdaGrad, Adam, RMSProp, момент Нестерова. {#2.09}
-------------------------------------------------------------------------------------

P.S. про градиентный спуск уже было в билете 2.2 (самое лучшее
объяснение за 7 минут - [здесь видос на
ютубе](https://www.youtube.com/watch?v=mdKjMPmcWjY) )\
Формула для градиентного спуска, $\alpha$ это шаг

For every epoch:\
$x := x - \alpha \cdot f'(x)$\

**SGD (стохастический градиентный спуск)** - то же самое, что и
градиентный спуск, но градиентный используется для каждой эпохи, в то
время как SGD - для каждый эпохи и каждого сэмпла. Проблема - слишком
много прыжков от точки минимума может произойти.\
**момент Нестерова**\
Сама по себе идея методов с накоплением импульса до очевидности проста:
«Если мы некоторое время движемся в определённом направлении, то,
вероятно, нам следует туда двигаться некоторое время и в будущем». Для
этого нужно уметь обращаться к недавней истории изменений каждого
параметра. Простое накопление импульса уже даёт хороший результат, но
Нестеров идёт дальше и применяет хорошо известную в вычислительной
математике идею: заглядывание вперёд по вектору обновления. (берет
производную не в точке $\gamma v_{t-1}$, а в $x - \gamma v_{t-1}$)

for every epoch t:\

$$v_t = v_{t-1} + \eta f^' (x - \gamma v_{t-1})$$
$$x_{t+1} = x_t - v_t$$

для краткости: далее $f'(x_t)$ равен $g_t$\
**AdaGrad**\
Хорошо бы уметь обновлять параметры с оглядкой на то, насколько типичный
признак они фиксируют. Достичь этого несложно: давайте будем хранить для
каждого параметра сети сумму квадратов его обновлений. Она будет
выступать в качестве прокси для типичности: если параметр принадлежит
цепочке часто активирующихся нейронов, его постоянно дёргают туда-сюда,
а значит сумма быстро накапливается. Перепишем формулу обновления вот
так:

for every epoch t:\

$$G_t = G_{t-1} + g_{t}^{2}$$
$$x_{t+1} = x_t - \frac{\alpha}{\sqrt{G_t + \epsilon}}g_t$$

Где $G_{t}$ ---- сумма квадратов обновлений, а $\epsilon$ ---
сглаживающий параметр, необходимый, чтобы избежать деления на
0.Недостаток Adagrad в том, что $G_{t}$ может увеличиваться сколько
угодно, что через некоторое время приводит к слишком маленьким
обновлениям и параличу алгоритма.\
**RMSProp** - модификация Adagard, мы всё так же собираемся обновлять
меньше веса, которые слишком часто обновляются, но вместо полной суммы
обновлений, будем использовать усреднённый по истории квадрат градиента.

for every epoch t:\

$$E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g_t^2$$
$$x_{t+1} = x_t - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}}g_t$$\
**Adam** Объединяет моменты Нестерова и RMSProp.

for every epoch t:\

$$E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g_t$$
$$G_t = \beta G_{t-1} + (1-\beta)g_t^2$$
$$x_{t+1} = x_t - \frac{\alpha}{\sqrt{G_t + \epsilon}}E[g^2]_t$$
