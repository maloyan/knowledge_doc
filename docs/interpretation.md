Методы интерпретации нейронных сетей. Градиентные методы: GradCAM, Integrated gradients, Noise Tunnel. Методы на основе механизма внимания: матрица внимания, CLEAR, SCOUTER.
------------------------------------------------------------------- 

## Методы интерпретации нейронных сетей

Нейронные сети представляют собой сложные математические модели, которые могут обучаться и совершать прогнозы на основе большого количества данных. Однако одним из недостатков таких моделей является их "черный ящик" - природа, которая делает интерпретацию их предсказаний сложной задачей. Чтобы решить эту проблему, были разработаны различные методы интерпретации нейронных сетей. 

## Градиентные методы

### GradCAM (Gradient-weighted Class Activation Mapping)

GradCAM использует градиенты, выходящие из последнего сверточного слоя в сети, чтобы понять, какие особенности на изображении были важны для прогноза.

Для вычисления GradCAM используется следующая формула:

$$
L_{GradCAM}^{c} = ReLU\left(\sum_{k} \alpha_k^c . A^k\right)
$$

где $A^k$ - активация $k$-го канала, $\alpha_k^c$ - веса, определяемые градиентами класса $c$ по отношению к каналу $k$ (вычисляются путем глобального усреднения градиентов), а $ReLU$ обеспечивает сохранение только позитивных вкладов.

### Integrated Gradients

Integrated Gradients, предложенный Sundararajan и соавторами, объясняет разницу между выходами модели на базовом и целевом входе через интеграцию градиентов модели вдоль пути от базового до целевого входа.

Математически метод определен следующим образом:

$$
\text{Integrated Gradients}_i(x, x') = (x_i - x'_i) \cdot \int_{\alpha=0}^1 \frac{\partial F(x' + \alpha \cdot (x - x'))}{\partial x_i} d\alpha
$$

где $x$ - целевой вход, $x'$ - базовый вход, $F$ - модель, а $i$ обозначает $i$-й элемент входа.

### Noise Tunnel

Noise Tunnel - это метод, который используется для оценки неопределенности в интерпретации модели путем добавления шума к входным данным вдоль пути интеграции. Он может быть применен к любому методу атрибуции.

Метод атрибуции, модифицированный при помощи Noise Tunnel, может быть записан следующим образом:

$$
\text{Attribution}_{\text{NoiseT

unnel}}(x) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma^2)}[\text{Attribution}(x + \epsilon)]
$$

где $x$ - входные данные, $\text{Attribution}(x)$ - атрибуция оригинального входа, а $\mathcal{N}(0, \sigma^2)$ - гауссовский шум с нулевым средним и стандартным отклонением $\sigma$.

## Методы на основе механизма внимания

### Матрица внимания

Матрица внимания используется в трансформерных моделях и позволяет модели сосредоточиться на определенных частях входных данных при создании выходных данных. 

Матрица внимания вычисляется следующим образом:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

где $Q$, $K$ и $V$ - матрицы запросов, ключей и значений соответственно, а $d_k$ - размерность ключей.

### CLEAR (CLass-Enhanced Attentive Response)

CLEAR - это механизм, который улучшает процесс аттеншена, вводя классовые сигналы в матрицу внимания. 

Формула для вычисления CLEAR представлена ниже:

$$
\text{Attention}_{\text{CLEAR}}(Q, K, V, C) = \text{softmax}\left(\frac{QK^T + QC^T}{\sqrt{d_k}}\right)V
$$

где $C$ - классовый сигнал.

### SCOUTER (Score-COUnt-based aTtEntion for deep Rule)

SCOUTER - это метод интерпретации моделей глубокого обучения, который строит условные правила с помощью аттеншен-механизма, основанного на подсчете оценок. Этот метод основан на гипотезе, что важность признака должна быть выше, если он часто активируется при предсказании определенного класса.

SCOUTER использует следующую формулу для расчета важности:

$$
\text{Importance}(F, c) = \sum_{i=1}^{N} \text{Count}(F_i, c) \cdot \text{Score}(F_i, c)
$$

где $F$ - признак, $c$ - класс, $N$ - количество входных образцов, $\text{Count}(F_i, c)$ - количество активаций признака $F_i$ при предсказании класса $c$, а $\text{Score}(F_i, c)$ - средний вклад признака $F_i$ в предсказание класса $c$

.
