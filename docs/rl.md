Обучение с подкреплением. Основные элементы: среда, агент, функция награды, действия. Монте-Карло, Temporal difference. Проблема исследования и эксплуатации (exporation&expoitation). Алгоритм DQN.
------------------------------------------------------------------- 

# Обучение с подкреплением

Обучение с подкреплением (Reinforcement Learning, RL) - это область машинного обучения, в которой агент обучается, совершая действия в определённой среде, чтобы максимизировать некоторую форму награды.

**Среда** (environment) - это мир, в котором агент действует. Она реагирует на действия агента и возвращает новое состояние и награду.

**Агент** (agent) - это сущность, которая взаимодействует со средой, выбирает действия на основе своей политики.

**Функция награды** (reward function) - это функция, которая определяет, какую награду агент получит в результате выполнения определенного действия в определенном состоянии. Цель агента - максимизировать сумму наград.

**Действия** (actions) - это возможные действия, которые агент может предпринять в данном состоянии.

## Методы оценки политики: Монте-Карло и Temporal Difference

**Метод Монте-Карло** (Monte Carlo) для обучения с подкреплением использует сэмплирование для оценки ожидаемого возвращаемого значения (return) для каждого состояния, экшена или пары состояние-экшен. Идея заключается в том, чтобы играть большое количество эпизодов и усреднять полученные возвращаемые значения.

**Temporal Difference** (TD) - это класс методов, которые комбинируют идеи Монте-Карло и итерации по состоянию. Основное отличие TD от Монте-Карло в том, что TD не ждет окончания эпизода, чтобы обновить оценку возвращаемого значения.

## Проблема исследования и эксплуатации (Exploration and Exploitation)

При обучении с подкреплением агенту необходимо сбалансировать **исследование** (exploration) новых действий и **эксплуатацию** (exploitation) уже известных действий, которые принесли хорошие результаты. Если агент только исследует, он может не найти оптимальную стратегию, так как он постоянно пробует что-то новое. Если агент только эксплуатирует, он может упустить возможность улучшить свои действия, так как он не исследует новые стратегии.

## Алгоритм DQN (Deep Q-Network)

DQN - это алгорит

м обучения с подкреплением, который использует глубокое обучение для оценки Q-функции. Q-функция определяет ожидаемую награду за каждое действие в каждом состоянии. В DQN используется нейронная сеть для аппроксимации Q-функции.

Основные идеи в DQN:
- **Experience Replay**: Вместо обучения на последовательных действиях, DQN хранит историю переходов и обучается на случайно выбранных примерах из истории. Это помогает снизить корреляцию между последовательными обучающими примерами и улучшить процесс обучения.
- **Target Network**: DQN использует две одинаковые нейронные сети. Одна сеть обновляется на каждом шаге обучения, а другая - значительно реже. Это помогает стабилизировать процесс обучения.

В общем виде уравнение Q-функции для DQN выглядит так:
\[
Q(s, a) = r + \gamma \max_{a'} Q'(s', a')
\]

где:
- \(s\) и \(a\) - текущее состояние и действие
- \(r\) - награда за совершенное действие
- \(\gamma\) - коэффициент дисконтирования, определяющий, насколько важны будущие награды по сравнению с текущей
- \(s'\) и \(a'\) - следующее состояние и действие
- \(Q'\) - Q-функция для target network.