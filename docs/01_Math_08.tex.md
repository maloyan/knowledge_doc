Понятие выборки и генеральной совокупности. Доверительный интервал. Функция правдоподобия. Метод максимального правдоподобия. EM-алгоритмы. {#1.08}
-------------------------------------------------------------------------------------------------------------------------------------------

В целом основная задача математической статистики это поиск
распределения по выборке, а-ля подгоните наш эксперимент так, чтобы его
можно было понять какой это вид распределения и вероятностно обсчитать.

Обычно мы от распределения не знаем какой-то числовой параметр, чтобы
его оценить, пользуются известной нам функцией - статистикой $T(X)$. И
да, скорее правильно сказать не выборка из P, а выборка с распределением
P.\

![Пример нахождения параметров для нормального распределения для
описания эксперимента](images/mlenormal.png){width="80%"}

**Генеральная совокупность** -- это совокупность всех мысленно возможных
объектов данного вида, над которыми проводятся наблюдения с целью
получения конкретных значений определенной случайной величины.

**Выборкой (выборочной совокупностью)** называется совокупность случайно
отобранных объектов из генеральной совокупности.

**Статистикой** $T(X)$ называется любая случайная величина, являющаяся
функцией лишь от выборки $X$.

Пусть у нас есть параметрическое семейство распределений
$\{P_\theta | \theta \in \Theta\}$.\
\
**Определение 4**. Пусть $X=(X_1,\dots,X_n)$ --- выборка из
$P \in \{P_\theta | \theta \in \Theta\}$ и $\Theta \subset \mathbb{R}$.
*Доверительным интервалом* уровня $\gamma \in (0, 1)$ для параметра
$\theta$ называется интервал $(T_1(X), T_2(X))$, что
$$\forall \theta \in \Theta \;\; P_\theta (T_1(X) \leq \theta \leq T_2(X)) \geq \gamma$$
То есть доверительный интервал - это такой интервал, что вероятность
принадлежности параметра $\theta$ больше $\gamma$.

Пример. Пусть у нас есть семейство параметрических нормальных
распределений $\mathcal{N}(\theta, \sigma^2)$, то есть наш параметр -
это среднее. Обычно доверительные интервалы используют двумя способами:
фиксируют $\theta$ или $\gamma$. Если мы фиксируем $\theta$, то мы хотим
узнать с какой вероятностью наш параметр (в примере среднее) принадлежит
доверительному интервалу. Если мы фиксируем $\gamma$, мы хотим оценить в
каком доверительном интервале лежит наш параметр с заданной
вероятностью(обычно эта вероятность большая $\sim 95\%$).

**Определение 5**. Пусть $X=(X_1,\dots,X_n)$ --- выборка из
$P \in \{P_\theta | \theta \in \Theta\}$, $x = (x_1, \dots, x_n)$ -
реализация выборки $X$. Если $X$ имеет дискретное распределение, то
определим функцию $L(x, \theta)$:
$$L(x, \theta) = \prod\limits_{i=1}^{n} P(X_i = x_i)$$ Если $X$ имеет
непрерывное распределение с плотностью $p(x, \theta)$, то определим
функцию $L(x, \theta)$:
$$L(x, \theta) = \prod\limits_{i=1}^{n} p(x_i, \theta)$$ Функция
$L(x, \theta)$ называется *функцией правдоподобия*.

**Определение 6**. Пусть $X=(X_1,\dots,X_n)$ --- выборка из
$P \in \{P_\theta | \theta \in \Theta\}$ и $\Theta \in \mathbb{R}$ и
$L(x, \theta)$ - функция правдоподобия. *Оценкой максимального
правдоподобия* $\hat{\theta}$ называется такая точка параметрического
множества $\Theta$, в которой функция максимального правдоподобия
$L(x ,\theta)$ при заданном $x$ достигает максимума:
$$L(x, \hat{\theta}) = \sup_{\theta \in \Theta} L(x, \theta)$$\
Для максимизации функции правдоподобия обычно берут ее логарифм, т.к.
вычисления будут проще.

**Определение 7**.*EM-алгоритм* (expectation-maximization) - алгоритм
для нахождения оценок максимального правдоподобия параметров, в случае,
когда модель зависит от некоторых скрытых переменных.

Как правило, ЕМ-алгоритм применяется для решения задач двух типов.

-   задачи, связанные с анализом неполных данных, когда некоторые
    статистические данные отсутствуют в силу каких-либо причин.

-   задачи, в которых функцию правдоподобия сложно максимизировать, но
    это можно сделать, если в задачу искусственно ввести дополнительные
    переменные.

Рассмотрим подробнее на примере. Пусть у нас есть смесь двух гауссовых
распределений. $q$ - вероятность первого гауссового распределения,
$1 - q$ - вероятность второго гауссового распределения.
$f(x|\mu_1, \sigma_1^2)$ - плотность первого гауссового распределения,
$f(x|\mu_2, \sigma_2^2)$ - плотность второго гауссового распределения.

![Смесь двух гауссовых распределений](images/mixture.png)

Плотность смеси распределений будет иметь вид:
$$p(x|q, \mu_1, \sigma_1, \mu_2, \sigma_2) = q \cdot f(x|\mu_1, \sigma_1^2) + (1-q) \cdot f(x|\mu_2, \sigma_2^2)$$

Применим к этой функции метод максимального правдоподобия:
$$\begin{gathered}
    L(X|q, \mu_1, \sigma_1, \mu_2, \sigma_2) = \\
    \prod\limits_{i=1}^{n}p(x_i|q, \mu_1, \sigma_1, \mu_2, \sigma_2) = \\
    \prod\limits_{i=1}^{n}  \big( q \cdot f(x_i|\mu_1, \sigma_1^2) + (1-q) \cdot f(x_i|\mu_2, \sigma_2^2) \big)\end{gathered}$$

Данное выражение будет сложно оптимизировать напрямую. Добавим
искусственно скрытые переменные для решения нашей задачи. Т.е. перейдем
от $X = \{x_i\}_{i=1}^n \Rightarrow \{x_i, z_i\}_{i=1}^n$, где $z_i$ -
индикатор i-ой гауссианы.

$$\begin{gathered}
    L(X, Z|q, \mu_1, \sigma_1, \mu_2, \sigma_2) = \prod\limits_{i=1}^{n}p(x_i, z_i|q, \mu_1, \sigma_1, \mu_2, \sigma_2) = \\
    \prod\limits_{i=1}^{n}p(z_i|q, \mu_1, \sigma_1, \mu_2, \sigma_2) = p(x_i|z_i, q, \mu_1, \sigma_1, \mu_2, \sigma_2) = \\
    \prod\limits_{i=1}^{n} q^{z_i} \cdot (1-q)^{1-z_i} \cdot f(x_i|\mu_1, \sigma_1^2)^{z_i} \cdot f(x_i|\mu_2, \sigma_2^2)^{1-z_i}\end{gathered}$$

Полученное выражение нужно максимизировать. Если мы возьмем логарифм,
как в обычном методе максимального правдоподобия и будем максимизировать
его, беря производную, то найдем все параметры.

Но мы не знаем $z$, откуда его взять? Для этого как раз таки и нужен
EM-алгоритм.

Нам нужно найти $L(X, Z| \theta)$, где $X$ - видимые переменные, $Z$ -
скрытые переменные, $\theta$ - параметры.

-   На E-шаге (expectation) вычисляется ожидаемое значение функции
    правдоподобия, при этом скрытые переменные рассматриваются как
    наблюдаемые: фиксируем $\theta$ и ищем математическое ожидание
    $E(Z)$(для каждого случая поиск мат. ожидания индивидуальный)

-   На M-шаге (maximization) вычисляется оценка максимального
    правдоподобия, таким образом увеличивается ожидаемое правдоподобие,
    вычисляемое на E-шаге: фиксируем математическое ожидание $E(Z)$ и
    ищем $L(X, Z| \theta)$, где $Z$ заменяем на $E(Z)$ и максимизируем
    по $\theta$. Затем это значение используется для E-шага на следующей
    итерации.

Эти шаги мы проводим до сходимости.
