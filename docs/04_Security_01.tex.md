Базовые принципы создания атакующих примеров. Классификация атак. Атаки уклонением. Атаки отравлением данных. Отравление модели, зашумление данных, порча меток. Инверсия модели. {#4.01}
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

*текста много, но суть проста, не бойтесь*

Опасность для систем машинного обучения (в цифровой среде) -- это прямое
или косвенное воздействие на элементы конвейера (пайплайна) систем
машинного обучения (МО) для того, чтобы добиться одного (или нескольких
эффектов):

-   Получить желаемое поведение системы МО

-   Воспрепятствовать правильной работе системы МО

-   Полностью исключить возможность работы систем МО

-   Получить непубличную информацию о системе МО и/или данных,
    использованных при обучении

Искусственное воздействие на элементы конвейера систем МО принято
называть атаками

Соответственно, чисто формально, атаки могут затрагивать (воздействовать
на) тренировочные данные, тестовые данные (данные периода исполнения или
реальные данные) и саму модель.

**Классификация атак**

  --------------------- --------------------------- ----------------------------
  **Атака**             **Место Атаки**             **Затрагиваемые параетры**
  Adversarial Attack    использование               Входные данные
  Backdoor Attack       тренировка                  Параметры сети
  Data poisining        тренировка, использование   Входные данные
  IP stealing           использование               Отклик системы
  Neural-level trojan   тренировка                  Отклик системы
  Hardware trojan       аппаратное проектирование   Отклик системы
  Side-channel Attack   использование               Отклик системы
  --------------------- --------------------------- ----------------------------

***Состязательные атаки*** -- подбор входных данных, которые
\"обманывают\" систему МО. Чаще всего представляются как специально
подобранные модификации корректно обрабатываемых данных. (например
данные с наложенным специальным шумом)

Остальные атаки (отравление данных,бэкдор) -- концептуально похожи на
классические проблемы кибербезопасности.

### Использование API

**Репликация модели**

Злоумышленник может воспроизвести функциональность модели машинного
обучения, используя ее API вывода. В этом случае репликации модели
злоумышленник неоднократно запрашивает API вывода жертвы, и использует
его в качестве идей для сбора комбинации данных и метки. Из комбинации
(данные, метка) злоумышленник строит теневую модель, которая эффективно
функционирует как модель жертвы, но с меньшей точностью. Обычно это
первый шаг к атакам уклонения.

**Кража модели**

Функциональные возможности моделей машинного обучения могут быть
украдены с использованием API вывода.

Разница между извлечением модели и репликацией модели: в атаках
извлечения модели злоумышленник может построить теневую модель, точность
которой совпадает с точностью модели жертвы, и, следовательно, атаки с
кражей / извлечением модели приводят к украденной интеллектуальной
собственности. В атаках репликации модели теневая модель не имеет такой
же точности, как модель жертвы.

### Атаки уклонением

В отличие от атак отравления, которым требуется доступ к обучающим
данным, злоумышленники могут обмануть классификатор ML, просто испортив
запрос к модели ML.

В более широком смысле -- входные данные, которые не позволяют модели
машинного обучения точно идентифицировать выборку данных. Этот метод
можно использовать для обхода модели машинного обучения. И здесь есть
два режима -- offline и online.

**Offline уклонение**

Есть автономная копия модели МО, полученная с помощью репликации модели
или извлечения модели - в зависимости от конкретного случая автономная
копия может быть теневой копией или точной реконструкцией исходной
модели.

Благодаря наличию offline копии модели, учимся уклоняться от модели МО,
не опасаясь раскрытия. Нашли образец для уклонения, злоумышленник может
по существу воспроизвести образец на реальной модели.

Как алгоритмически найти образец, который уклоняется от автономной
модели машинного обучения?

Существует множество стратегий выбора данных для уклонения, и в
зависимости от экономики (собственных затрат): простое преобразование
входных данных (обрезка, сдвиг, перевод), обычное искажение (добавление
белого шума в фон), состязательные примеры (специально вычисленные
возмущения ввода для достижения желаемого результата) и специальные
данные, которые вызывают предопределенный отклик.

**Online уклонение**

Те же самые вспомогательные техники, как простое преобразование входных
данных (обрезка, сдвиг, перевод), обычное искажение (добавление белого
шума в фон), состязательные примеры (специально вычисленные возмущения
ввода для достижения желаемого результата) и специальные данные, которые
вызывают предопределенный отклик также работают и в контексте
онлайн-атак с уклонением. Различие между Offline и Online заключается в
том, что в первом случае атакованная модель либо украдена /
реплицирована, а во втором случае - это живая модель машинного обучения.
Отметим также, что запросы к реальной системе могут быть и затратными
(например, подбор уклонения для некоторого платного сервиса).

### Неверные модели

Суть:

Все модели машинного обучения существуют в виде кода и поэтому уязвимы
для «традиционных программных атак». Ряд популярных пакетов, таких как
Tensorflow, Caffe, OpenCV, имеют открытые CVE (Common Vulnerabilities
and Exposures), что делает их уязвимыми для традиционных атак на
переполнение кучи и переполнение буфера.

Часто используют предобученные модели, тем самым подложив \"взломанную\"
модель или данные в открытый источник можно получить требуемый результат
при атаке. (Предостережение, нужно брать модели и датасеты всегда из
проверенных источников)

Также это всё может быть провернуто через внедрение в образы облачных
контейнеров вредоносного кода. Amazon Web Service (AWS), образы Google
Cloud Platform (GCP) и образы Azure, а также популярные среды выполнения
контейнеров, такие как Docker.

### Отравление модели МО

Злоумышленники могут обучать модели машинного обучения, которые являются
производительными, но содержат бэкдоры, которые приводят к ошибкам
вывода при представлении входных данных, содержащих триггер,
определенный самим злоумышленником.

Модель с бэкдором может быть представлена конечным пользователям (как
говорилось в подсекции *Неверные модели*) через предварительно обученную
модель с бэкдором или как результат отравления данных. Эта отравленная
модель может быть использована во время вывода с помощью атаки
уклонения.

### Отравление данных

Злоумышленники могут попытаться отравить наборы данных, используемые
системой машинного обучения, путем изменения базовых данных или их
меток. Это позволяет злоумышленнику внедрять уязвимости в модели
машинного обучения, обученные на данных, которые может быть трудно
обнаружить. Встроенную уязвимость можно активировать позже, предоставив
модели данные, содержащие триггер. Отравление данных может помочь
включить атаки уклонением.

**Загрязнение данных**

Злоумышленники могут попытаться добавить свои собственные данные в набор
данных с открытым исходным кодом, что может создать бэкдор
классификации. Например, злоумышленник может вызвать целевую атаку с
ошибочной классификацией только тогда, когда в запросе присутствуют
определенные триггеры. В остальных случая система будет работать честно.

**Зашумление данных**

Добавление шума к набору данных снизит точность модели, потенциально
делая модель более уязвимой для неправильной классификации.

**Порча меток**

Аналогично с Загрязнением данных, только портим метки.

Злоумышленники могут попытаться изменить метки в обучающем наборе, что
приведет к неправильной классификации модели. Изменение обучающих меток
может создать бэкдор в модели, так что злонамеренный ввод всегда будет
классифицироваться в пользу злоумышленника. Например, злоумышленник
может вызвать целевую атаку с ошибочной классификацией только тогда,
когда в запросе присутствуют определенные триггеры. В остальных случая
система будет работать правильно.

### Инверсия модели МО

Под «инверсией модели» понимают возможность получения обучающих данных
из обученной модели.

Данные обучения моделей машинного обучения можно реконструировать,
используя оценки достоверности, доступные через API вывода. Опрашивая
результаты вывода, злоумышленник может получить обратно потенциально
конфиденциальную информацию, встроенную в обучающие данные. Это может
привести к нарушениям конфиденциальности, если злоумышленник сможет
восстановить данные конфиденциальных функций, используемых в алгоритме.
