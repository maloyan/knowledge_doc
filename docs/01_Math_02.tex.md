Математическое ожидание случайной величины. Дисперсия случайной величины. Ковариация случайных величин. Матрица ковариаций вектора случайных величин. Дисперсия суммы независимых случайных величин. {#1.02}
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Пусть $\xi$ - абсолютно непрерывная случайная величина, $p(x)$ - ее
плотность распределения. Тогда ее **математическим ожиданием**
называется величина
$$\mathbb{E}(\xi) = \int\limits_{-\infty}^{\infty} x \cdot p(x) dx$$

Свойства математического ожидания:

-   $\mathbb{E}(a\xi + b\eta) = a \cdot \mathbb{E}(\xi) + b \cdot \mathbb{E}(\eta)$

-   $\xi = const = C \Rightarrow \mathbb{E}(\xi) = C$

-   если $\xi$ и $\eta$ независимы, то
    $\mathbb{E}(\xi \eta) = \mathbb{E}(\xi) \cdot \mathbb{E}(\eta)$

**Математическое ожиданием в дискретном** случае можно посчитать как
$$\mathbb{E}(\xi)=\sum\limits_{x \in X} x \cdot p(\xi = x)$$

**Дисперсией** случайной величины называют математическое ожидание
квадрата отклонения случайной величины от её математического ожидания.
Пусть $\xi$ - случайная величина, тогда дисперсией называется:
$$D(\xi) = E(\xi - E(\xi))^2$$

Свойства дисперсии:

-   $D(\xi) \geq 0$

-   $D(a\xi) = a^2 D(\xi)$

-   $D(\xi + const) = D(\xi)$

-   $D(const) = 0$

-   $D(\xi) = \mathbb{E}(\xi^2) - \mathbb{E}^2(\xi)$

-   если $\xi$ и $\eta$ независимы, то
    $D(\xi + \eta) = D(\xi) + D(\eta)$

![Знак ковариации двух случайных величин $X$ и
$Y$](images/covariance.png)

Пусть $\xi$ и $\eta$ - случайные величины. Тогда **ковариация**
определяется как
$$cov(\xi, \eta) = \mathbb{E}\big[(\xi - \mathbb{E}(\xi)) \cdot (\eta - \mathbb{E}(\eta))]$$

Ковариация случайной величины с собой является дисперсией
$cov(\xi, \xi) = D(\xi)$

**Корреляцией** случайных величин $\xi$ и $\eta$ называется число
$$corr(\xi, \eta) = \frac{cov(\xi, \eta)}{\sqrt{D(\xi) \cdot D(\eta)}}$$
Корреляция принимает значения от -1 до 1, и помимо направления
зависимости она также показывает меру зависимости между случайными
величинами, насколько сильно они зависят друг от друга.

Если у нас есть вектор случайных величин $(\xi_1, \xi_2, \dots, \xi_n)$,
то **матрица ковариаций** этого вектора выглядит: $$\begin{bmatrix}
            cov(\xi_1, \xi_1) & cov(\xi_1, \xi_2) & \dots & cov(\xi_1, \xi_n) \\
            cov(\xi_2, \xi_1) & cov(\xi_2, \xi_2) & \dots & cov(\xi_2, \xi_n) \\
            \dots             & \dots             & \dots & \dots             \\
            cov(\xi_n, \xi_1) & cov(\xi_n, \xi_2) & \dots & cov(\xi_n, \xi_n) \\
        \end{bmatrix}$$

В многомерном пространтсве по этой матрице можно понять попарную
ковариацию (т.е. положительно или отрициательно зависят друг от друга
случайные величины). А по диагональным элементам понять дисперсию
случайной величины

![\"Визуализация матрицы ковариации\" на примере свойств растений. На
диагонали распределение случайных величин, по которым можно оценить
дисперсию. А по недиагональным графикам можно понять положительно или
отрицательно зависят друг от друга случайные
величины](images/covariance_vizualization.png)
