Теория информации. Энтропия по Шеннону. {#1.07}
---------------------------------------

Менее вероятное событие более информативно. Например, сообщение
\"сегодня утром выглянуло солнце\" гораздо менее информативно (оно
обычное, для нас это не новая информация), чем \"сегодня утром было
солнечное затмение\". Сообщение, вероятность которого равна 1, несёт
нулевую информацию(информация о том, что трава зеленая, нам ничего не
дает).

Основываясь на идее о том, что прирост информации равен утраченной
неопределённости, Шеннон выдвинул разумные требования для функции
энтропии $H$:

1.  мера должна быть непрерывной;

2.  в случае, когда все варианты равновероятны, увеличение количества
    вариантов должно всегда увеличивать значение функции;

3.  должна быть возможность сделать выбор в два шага, в которых значение
    функции конечного результата должно являться суммой функций
    промежуточных результатов.

**Собственной информацией** случайной величины $x$ с плотностью
распределения $p_{X}(x)$ называется число $$I(x) = − \log{p_{X}(x)}$$,
$log$ можно брать по разным основаниям. Если основание = 2, то единица
измерения количества информации называется «бит», если берётся
натуральный логарифм, то - «нат».

Собственная информация сама является случайной величиной определённая
для множества сообщений $X =
        x_1, \dots , x_n$ с распределением вероятностей
$P = p(x_1), \dots , p(x_n)$, которую следует отличать от её среднего
значения --- информационной энтропии.

Информация аддитивна для статистически независимых сообщений:
$I(x_1, x_2) = I(x_1) + I(x_2)$.

*Доказательство*. Действительно, если сообщения независимы, то
$p(x_1, x_2) = p(x_1) \cdot p(x_2)$, а следовательно:
$$I(x_1, x_2) = −\log p(x_1, x_2) = −\log ( p(x_1) \cdot p(x_2) ) = −\log p(x_1) − \log p(x_2) = I(x_1) + I(x_2) \blacksquare$$

Математическое ожидание этой случайной величины $I(x)$ называется
**энтропией**:
$$H(x) = \mathbb{E}(I(x)) = -\sum\limits_{x \in X} p_{X}(x) \cdot \log{p_{X}(x)}$$

Простыми словами энтропия - это мера неопределённости. Если у нас много
информации, то у нас меньше неопределенности, следовательно, меньше
энтропия, и наоборот.

**Перекрестная энтропия(или кросс-энтропия)** - ожидаемая длина
закодированных сообщений в битах при использовании распределения $Q(x)$
вместо $P(x)$: $$H(P, Q) = -\sum\limits_{x \in X} P(x) \log Q(x)$$

Ее можно рассматривать как меру разницы между двумя распределениями.
Перекрестная энтропия уже широко используется в глубоком обучении в
качестве функции потерь для включения обучения. В этом случае истинное
распределение $P(x)$ вероятностей - это метка, а прогнозируемое
распределение $Q(x)$- это значение из текущей модели.

Пусть $P(x)$ - истинное распределение вероятностей сообщений, а $Q(x)$ -
прогнозируемое распределение вероятностей сообщений.

Измерить схожесть одного распределения $P$ с другим распределением $Q$
позволяет **дивергенция Кульбака-Лейблера (KL-дивергенция)**:
$$KL(P\|Q) = -\sum\limits_{x \in X} P(x) \log Q(x) + \sum\limits_{x \in X} P(x) \log P(x) = \sum \limits_{x \in X} P(x) \log\frac{P(x)}{Q(x)}$$
Это значение можно понимать как количество неучтённой информации
распределения $P(x)$, если бы $Q(x)$ было использовано для приближения
$P(x)$. Здесь $P(x)$ рассматривается как априорное распределение, $Q(x)$
как проверяемое.

KL-дивергенция неотрицательна и измеряет различие между 2
распределениями, но эта мера - не расстояние, т.к. не является
симметричной (не всегда $KL(P\|Q) = KL(Q\|P)$).

![Пример распределения $p$ и $q$ и их
$KL$-дивергенции](images/kl_example.png){width="80%"}
