Матричные разложения. Сингулярное разложение. {#1.09}
---------------------------------------------

P.S. в лекциях упоминается только спектральное и SVD (сингулярное)
разложение матриц, поправьте, если что-то упустила\
Несколько типов матриц, которые понадобятся нам в дальнейшем:

1.  Единичная $E$ - на главной диагонали стоят 1, в остальных местах 0.

2.  Транспонированная $A^\intercal$ - все элементы матрицы
    переставляются симметрично относительно главной диагонали.

3.  Обратная матрица $A^{−1} : AA^{−1} = A^{−1}A = E$

4.  Унитарная матрица $A : AA^\intercal = A^\intercal{A} = E$ Строки и
    столбцы такой матрицы образуют ортонормированный базис, а
    преобразование, заданное унитарной матрицей, сохраняет длины всех
    векторов и скалярное произведение.\

**Определение 1** Определитель матрицы $A - \det{A}$ - одна из
характеристик матрицы. Формула для нахождения определителя через
разложение по $i$-й строке:
$$\det{A} = a_{i1}A_{i1} + a_{i2}A_{i2} + \cdots  + a_{in}A_{in}$$

где алгебраическое дополнение $A_{ij} = (-1)^{i+j}\det{M}_{ij}$, а
подматрица $M_{ij}$ получается вычеркиванием $i$-й строчки и $j$-го
столбца из матрицы $A_{ij}$.

(Получается, что модуль определителя $\left|\det{A}\right|$ --- это
коэффициент изменения объекта при преобразовании пространства матрицей
A: для одномерного пространства --- длина отрезка, для плоскости ---
площадь фигуры, для трёхмерной фигуры --- её объём.)

**Определение 2** Ранг матрицы --- это истинная (максимальная)
размерность пространства, для которого определитель не равен нулю.

**Определение 3** Если $\det{A} = 0$, то матрица вырождена (убирает
объём). Если $\det{A} \neq 0$, то матрица обратима.

**Определение 4** Собственный вектор -- вектор не меняющий своё
направление при преобразованиях (а просто увеличивающийся на коэффициент
- собственное значение): $A\vec{v} = \lambda\vec{v}$, где
$\vec{v} \neq 0$

Метод поиска собственных векторов:
$$A\vec{v} = \lambda\vec{v} \quad A\vec{v} - \lambda\vec{v}=0 \quad A\vec{v} - \lambda\vec{v}E = 0 \quad (A\vec{v} - \lambda{E})\vec{v} = 0$$
$$\det{(A - \lambda{E})}\vec{v} = 0, \quad \vec{v} \neq 0 \implies \det{(A - \lambda{E})} = 0$$\
/\*А теперь про разложение, самое важное\*/\
\
**Определение 5** Спектральное разложение - разложение матрицы $A$ вида
$A = VLV^{−1}$ или $AV = VL$, где $V$ - матрица, в столбцах которой
стоят собственные векторы, $L$ - диагональная матрица, на диагонали
которой стоят собственные значения. Зачем это делать - хочется для
удобства разложить матрицы на более простые объекты (аналог --
разложение числа на простые множители).\
Норма вектора:
$$\left\|x \right\| = \left(\sum _{i}\left|x_{i}\right|^{2}  \right)^{\frac{1}{2}}$$

Норма матрицы:
$$\left\|A\right\| = \max_{1\le j\le n} \sum_{i=1}^{m} \left|a_{ij}\right|$$

Норма Фробениуса для матриц:
$$\left\|A\right\|_{F} = \left\|A\right\|_{2} = \sqrt{ \sum_{ij} \left|a_{ij}\right|^2} = \sqrt{Tr A^\intercal{A}}$$

**Метод главных компонент** - это метод выбора подпространства меньшей
размерности с минимальной потерей данных. А сингулярное разложение
(Singular Value Decomposition) - метод, который используется для такого
выбора. У нас есть множество данных, представим их как точки в
пространстве, наша задача - найти такое подпростанство, «проекция на
которое» сохранит максимальный разброс точек. Потом приблизим эту
«проекцию» эллипсом, в который попадут максимальное количество точек.
Остальные точки воспринимаем как «шум».\
Алгоритм SVD-разложения матрицы $A_{m×n}$:

1.  Составляем матрицу $A^\intercal{A}$ и находим её собственные
    значения $\lambda_{1} \re \cdots \re \lambda_{n}$, находим ненулевые
    сингулярные числа $\sigma_{i} = \sqrt{\lambda_{i}}$ и составляем из
    них диагональную матрицу $\Sigma$.

2.  Находим собственные векторы $v_{i}, i \in (1, \cdots, n)$ ,
    соответствующие значениям $\lambda_{i}$, производим нормирование
    каждого вектора. Ставим их как столбцы в матрицу $V$ и находим
    $V^\intercal$ .

3.  Строим векторы $u_{i} = \frac{Av_{i}}{\sigma_{i}}$ и дополняем
    любыми векторами до ортонормированного базиса
    $u_{i}, i \in (1, \cdots, m )$. Ставим их как столбцы в матрицу $U$.

4.  Записываем разложение $A = U\Sigma V^\intercal$ .
